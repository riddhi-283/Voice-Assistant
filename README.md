# ğŸ—£ï¸ Voice â†” Voice Chatbot (Streamlit + Whisper + Coqui TTS)

A lightweight, browserâ€‘based **voiceâ€‘toâ€‘voice** assistant without using any PAID APIs. Speak your question in the UI, the app transcribes it using **Groq Whisper**, gets an answer from a **Groq LLM (Llama 3.x)**, and speaks the reply back using **Coqui TTS** â€” all inside a simple Streamlit app.

---

## âœ¨ Features

* ğŸ™ï¸ **Mic-to-text** in the browser via `audio-recorder-streamlit`
* ğŸ§  **LLM chat** with Groq (e.g., `llama-3.1-8b-instant`, `llama-3.3-70b-versatile`)
* ğŸ—£ï¸ **Text-to-speech** via Coqui **Glow-TTS** (`tts_models/en/ljspeech/glow-tts`)
* ğŸ“ **Editable transcription** before sending
* ğŸ§¹ **Clear conversation** with one click
* ğŸ” Works with **GROQ\_API\_KEY** from sidebar or env var

---

## ğŸ§© How it works (high-level flow)

1. **Record audio** in the browser â†’ saved as wav
2. **Transcribe** using Groq Whisper (`whisper-large-v3-turbo`)
3. **Chat completion** via Groq LLM (model & temperature configurable)
4. **Synthesize voice** with Coqui TTS (Glowâ€‘TTS) â†’ WAV file
5. **Playback** assistant audio in Streamlit

---

## ğŸ“ Project structure

```
.
â”œâ”€ main.py                 # The Streamlit app 
â”œâ”€ requirements.txt       # Python dependencies
â””â”€ README.md              # This file
```

> **Note:** If your main script has a different name, replace `main.py` below accordingly.

---

## âœ… Prerequisites

* **Python** 3.9â€“3.11 (recommended: **3.10**)
* **FFmpeg** installed and on PATH
* Internet access to download model weights & call Groq API
* (Optional) **NVIDIA GPU** with a CUDA-compatible PyTorch build

### Install FFmpeg

* **macOS**: `brew install ffmpeg`
* **Ubuntu/Debian**: `sudo apt update && sudo apt install -y ffmpeg libsndfile1`
* **Windows**: Download from ffmpeg.org â†’ extract â†’ add `bin` folder to **PATH**

> Coqui TTS also uses `soundfile` (libsndfile). On Linux, we install `libsndfile1` above. If you see phonemizer/espeak warnings, install `espeak-ng` (`sudo apt install espeak-ng`).

---

## ğŸš€ Quickstart

### 1) Clone and enter the repo

```bash
git clone https://github.com/riddhi-283/Voice-Assistant.git
cd Voice-Assistant
```

### 2) Create & activate a virtual environment

**venv (crossâ€‘platform):**

```bash
py -3.10 -m venv .venv 
# Windows
.\.venv\Scripts\activate
# macOS / Linux
source .venv/bin/activate
```

### 3) Install dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

`requirements.txt` (for reference):

```txt
streamlit==1.37.0
groq==0.11.0
audio-recorder-streamlit==0.0.8
TTS==0.22.0
torch==2.1.0
soundfile==0.12.1
numpy==1.22.0
openai-whisper==20240930
ffmpeg-python==0.2.0
```

### 4) Set your Groq API key

Get your free API key from **groq.com** and set it either via the sidebar or as an env var.

* **macOS/Linux (bash/zsh):**

  ```bash
  export GROQ_API_KEY="YOUR_KEY"
  ```
* **Windows (PowerShell):**

  ```powershell
  $env:GROQ_API_KEY = "YOUR_KEY"
  ```

> You can also paste the key into the Streamlit **sidebar** field.

### 5) Run the app

```bash
streamlit run main.py
```

Then open the URL Streamlit prints (usually `http://localhost:8501`).

---

## ğŸ–±ï¸ Using the app

1. Enter/paste your **Groq API Key** in the sidebar (or rely on env var)
2. Pick **LLM Model** and **Temperature**
3. Click the **mic button** â†’ allow browser mic access â†’ speak
4. Review the **editable transcription**, make changes if needed
5. Click **Send**
6. Read the text reply and listen to the generated **voice response**
7. Use **ğŸ—‘ï¸ Clear Conversation** to reset the session

---

## ğŸ”§ Code tour (key functions)

* `get_client()` â€“ creates a Groq client using `GROQ_API_KEY`
* `llm_reply(messages, model_id, temperature)` â€“ calls Groq Chat Completions
* `synthesize_tts(text)` â€“ runs Coqui `glow-tts` â†’ writes a WAV to a temp folder
* `write_wav(bytes)` â€“ persists incoming audio bytes as a wav file
* UI wiring:

  * `audio_recorder(..., sample_rate=16000)` gathers mic input
  * On new audio â†’ Groq **Whisper** `audio.transcriptions.create(..., model="whisper-large-v3-turbo")`
  * Transcription saved to `st.session_state.pending_text` (editable `st.text_area`)
  * On **Send** â†’ add user msg â†’ call LLM â†’ TTS â†’ play audio & append to chat

